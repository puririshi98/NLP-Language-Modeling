{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h3>**Welcome to the Language modeling Notebook.**</h3></center>\n",
    "\n",
    "In this notebook, we train a neural network to **generate news headlines**.\n",
    "To reduce computational needs, we have reduced it to headlines about technology, and a handful of Tech giants.\n",
    "In this notebook you will:\n",
    "- Learn to preprocess raw text so it can be fed into an LSTM.\n",
    "- Make use of the LSTM library of Tensorflow, to train a Language model to generate headlines\n",
    "- Use your network to generate headlines, and judge which headlines are likely or not\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is a language model?**\n",
    "\n",
    "Language modeling is the task of assigning a probability to sentences in a language. Besides assigning a probability to each sequence of words, the language models also assigns a probability for the likelihood of a given word (or a sequence of words) to follow a sequence of words.\n",
    "— Page 105, __[Neural Network Methods in Natural Language Processing](https://www.amazon.com/Language-Processing-Synthesis-Lectures-Technologies/dp/1627052984/)__, 2017.\n",
    "\n",
    "In terms of neural network, we are training a neural network to produce probabilities (classification) over a fixed vocabulary of words.\n",
    "Concretely, we are training a neural network to produce:\n",
    "$$ P ( w_{i+1} | w_1, w_2, w_3, ..., w_i), \\forall i \\in (1,n)$$\n",
    "\n",
    "** Why is language modeling important? **\n",
    "\n",
    "Language modeling is a core problem in NLP.\n",
    "\n",
    "Language models can either be used as a stand-alone to produce new text that matches the distribution of text the model is trained on, but can also be used at the front-end of a more sophisticated model to produce better results.\n",
    "\n",
    "Recently for example, the __[BERT](https://arxiv.org/abs/1810.04805)__ paper show-cased that pretraining a large neural network on a language modeling task can help improve state-of-the-art on many NLP tasks. \n",
    "\n",
    "How good can the generation of a Language model be?\n",
    "\n",
    "If you have not seen the latest post by OpenAI, you should read some of the samples they generated from their language model __[here](https://blog.openai.com/better-language-models/#sample1)__.\n",
    "Because of computational restrictions, we will not achieve as good text production, but the same algorithm is at the core. They just use more data and compute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting, make sure you have all these libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (1.13.1)\n",
      "Requirement already satisfied: six>=1.10.0 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from tensorflow) (1.12.0)\n",
      "Requirement already satisfied: absl-py>=0.1.6 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from tensorflow) (0.7.1)\n",
      "Requirement already satisfied: tensorboard<1.14.0,>=1.13.0 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from tensorflow) (1.13.1)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from tensorflow) (1.0.7)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from tensorflow) (1.19.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from tensorflow) (0.33.1)\n",
      "Requirement already satisfied: tensorflow-estimator<1.14.0rc0,>=1.13.0 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from tensorflow) (1.13.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from tensorflow) (1.16.2)\n",
      "Requirement already satisfied: gast>=0.2.0 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from tensorflow) (0.2.2)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from tensorflow) (1.0.9)\n",
      "Requirement already satisfied: astor>=0.6.0 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from tensorflow) (0.7.1)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from tensorflow) (3.7.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow) (3.0.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow) (0.15.0)\n",
      "Requirement already satisfied: h5py in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from keras-applications>=1.0.6->tensorflow) (2.9.0)\n",
      "Requirement already satisfied: mock>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow) (2.0.0)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from protobuf>=3.6.1->tensorflow) (40.8.0)\n",
      "Requirement already satisfied: pbr>=0.11 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from mock>=2.0.0->tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow) (5.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import Counter\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import tokenizer\n",
    "\n",
    "root_folder = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure the dataset files are all in the `dataset` folder of the assignment.\n",
    "\n",
    " - If you are using this notebook locally: You should run the `download_data.sh` script.\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "Number of training samples: 88568\n",
      "Number of validation samples: 946\n"
     ]
    }
   ],
   "source": [
    "# This cell loads the data for the model\n",
    "# Run this before working on loading any of the additional data\n",
    "\n",
    "with open(root_folder+\"dataset/headline_generation_dataset_processed.json\", \"r\") as f:\n",
    "    d_released = json.load(f)\n",
    "\n",
    "with open(root_folder+\"dataset/headline_generation_vocabulary.txt\", \"r\") as f:\n",
    "    vocabulary = f.read().split(\"\\n\")\n",
    "w2i = {w: i for i, w in enumerate(vocabulary)} # Word to index\n",
    "unkI, padI, start_index = w2i['UNK'], w2i['PAD'], w2i['<START>']\n",
    "\n",
    "vocab_size = len(vocabulary)\n",
    "\n",
    "input_length = len(d_released[0]['numerized']) # The length of the first element in the dataset, they are all of the same length\n",
    "d_train = [d for d in d_released if d['cut'] == 'training']\n",
    "d_valid = [d for d in d_released if d['cut'] == 'validation']\n",
    "\n",
    "print(\"Number of training samples:\",len(d_train))\n",
    "print(\"Number of validation samples:\",len(d_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have loaded the data, let's inspect one of the elements. Each sample in our dataset is has a `numerized` vector, that contains the preprocessed headline. This vector is what we will feed in to the neural network. The field `numerized` corresponds to this list of tokens. The already loaded dictionary `vocabulary` maps token lists to the actual string. Use these elements to recover `title` key of entry 1001 in the training dataset.\n",
    "\n",
    "Here we write the numerized2text function and inspect element 1001 in the training dataset (`entry = d_train[1001]`).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reversing the numerized: microsoft donates cloud computing ' worth $ 1 bn ' PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD\n",
      "From the `title` entry: Microsoft donates cloud computing 'worth $1 bn'\n"
     ]
    }
   ],
   "source": [
    "def numerized2text(numerized):\n",
    "    \"\"\" Converts an integer sequence in the vocabulary into a string corresponding to the title.\n",
    "    \n",
    "        Arguments:\n",
    "            numerized: List[int]  -- The list of vocabulary indices corresponding to the string\n",
    "        Returns:\n",
    "            title: str -- The string corresponding to the numerized input, without padding.\n",
    "    \"\"\"\n",
    "    #####\n",
    "    # Wecover each word from the vocabulary in the list of indices in numerized, using the vocabulary variable\n",
    "    # We use the string.join() function to reconstruct a single string\n",
    "    \n",
    "    \n",
    "    words = []\n",
    "    converted_string = \"\"\n",
    "    s=' '\n",
    "    for num in numerized:\n",
    "        words.append(vocabulary[num])\n",
    "        \n",
    "    converted_string=s.join(words)\n",
    "    #####\n",
    "    \n",
    "    return converted_string\n",
    "\n",
    "entry = d_train[1001]\n",
    "print(\"Reversing the numerized: \"+numerized2text(entry['numerized']))\n",
    "print(\"From the `title` entry: \"+ entry['title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In language modeling, we train a model to produce the next word in the sequence given all previously generated words. This has, in practice, two steps:\n",
    "\n",
    "\n",
    "    1. Adding a special <START> token to the start of the sequence for the input. This \"shifts\" the input to the right by one. We call this the \"source\" sequence\n",
    "    2. Making the network predict the original, unshifted version (we call this the \"target\" sequence)\n",
    "\n",
    "    \n",
    "Let's take an example. Say we want to train the network on the sentence: \"The cat is great.\"\n",
    "The input to the network will be \"`<START>` The cat is great.\" The target will be: \"The cat is great\".\n",
    "    \n",
    "Therefore the first prediction is to select the word \"The\" given the `<START>` token.\n",
    "The second prediction is to produce the word \"cat\" given the two tokens \"`<START>` The\".\n",
    "At each step, the network learns to predict the next word, given all previous ones.\n",
    "    \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to write the build_batch function. Given a dataset, we select a random subset of samples, and will build the \"inputs\" and the \"targets\" of the batch\n",
    "\n",
    "Here write the build_batch function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<START>',\n",
       " 'UNK',\n",
       " 'PAD',\n",
       " 'to',\n",
       " ',',\n",
       " 'apple',\n",
       " 'facebook',\n",
       " 'google',\n",
       " \"'\",\n",
       " 'in',\n",
       " 'on',\n",
       " 'the',\n",
       " 'for',\n",
       " 'twitter',\n",
       " 'of',\n",
       " 'amazon',\n",
       " ':',\n",
       " 'and',\n",
       " 'a',\n",
       " 'with',\n",
       " 'microsoft',\n",
       " 'is',\n",
       " 'new',\n",
       " 'as',\n",
       " '$',\n",
       " 'says',\n",
       " 'over',\n",
       " 'after',\n",
       " 'its',\n",
       " \"apple's\",\n",
       " 'from',\n",
       " 'iphone',\n",
       " 'at',\n",
       " '?',\n",
       " 'by',\n",
       " 'it',\n",
       " \"google's\",\n",
       " '-',\n",
       " 'be',\n",
       " 'up',\n",
       " 'data',\n",
       " 'more',\n",
       " 'china',\n",
       " 'will',\n",
       " 'app',\n",
       " 'u.s.',\n",
       " 'how',\n",
       " 'you',\n",
       " \"facebook's\",\n",
       " 'that',\n",
       " 'ceo',\n",
       " 'are',\n",
       " 'million',\n",
       " '\"',\n",
       " 'users',\n",
       " 'about',\n",
       " 'trump',\n",
       " 'has',\n",
       " 'your',\n",
       " 'deal',\n",
       " 'out',\n",
       " 'billion',\n",
       " 'sales',\n",
       " 'samsung',\n",
       " \"amazon's\",\n",
       " 'may',\n",
       " 'an',\n",
       " 'ipad',\n",
       " 'not',\n",
       " 'eu',\n",
       " 'privacy',\n",
       " 'us',\n",
       " 'news',\n",
       " 'now',\n",
       " 'search',\n",
       " 'first',\n",
       " 'tv',\n",
       " 'into',\n",
       " 'service',\n",
       " 'launches',\n",
       " 'could',\n",
       " 'court',\n",
       " 'what',\n",
       " 'qualcomm',\n",
       " 'against',\n",
       " 'mobile',\n",
       " 'shares',\n",
       " 'android',\n",
       " 'tech',\n",
       " 'why',\n",
       " 'tax',\n",
       " 'case',\n",
       " 'can',\n",
       " 'patent',\n",
       " 'watch',\n",
       " ';',\n",
       " 'top',\n",
       " 'but',\n",
       " 'video',\n",
       " 'one',\n",
       " 'this',\n",
       " 'big',\n",
       " 'street',\n",
       " 'ads',\n",
       " 'down',\n",
       " 'buy',\n",
       " 'company',\n",
       " \"microsoft's\",\n",
       " 'just',\n",
       " 'pay',\n",
       " 'windows',\n",
       " 'gets',\n",
       " 'takes',\n",
       " 'launch',\n",
       " 'market',\n",
       " '&',\n",
       " 'cloud',\n",
       " 'stock',\n",
       " 'music',\n",
       " 'off',\n",
       " 'zuckerberg',\n",
       " 'store',\n",
       " 'said',\n",
       " '(',\n",
       " ')',\n",
       " 'than',\n",
       " 'phone',\n",
       " 'report',\n",
       " 'apps',\n",
       " 'ipo',\n",
       " 'get',\n",
       " 'antitrust',\n",
       " 'business',\n",
       " 'online',\n",
       " 'have',\n",
       " 'make',\n",
       " 'his',\n",
       " 'ad',\n",
       " 'next',\n",
       " 'plans',\n",
       " 'social',\n",
       " 'jobs',\n",
       " 'no',\n",
       " 'back',\n",
       " 'day',\n",
       " 'account',\n",
       " 'most',\n",
       " 'who',\n",
       " 'like',\n",
       " 'live',\n",
       " 'take',\n",
       " 'help',\n",
       " 'chief',\n",
       " \"it's\",\n",
       " 'internet',\n",
       " '1',\n",
       " 'fake',\n",
       " 'use',\n",
       " '5',\n",
       " 'web',\n",
       " 'people',\n",
       " 'year',\n",
       " '10',\n",
       " 'adds',\n",
       " 'uk',\n",
       " 'wall',\n",
       " 'post',\n",
       " 'unveils',\n",
       " 'fight',\n",
       " 's',\n",
       " 'all',\n",
       " 'india',\n",
       " 'some',\n",
       " 'free',\n",
       " 'tablet',\n",
       " 'plan',\n",
       " 'was',\n",
       " 'profit',\n",
       " 'security',\n",
       " 'africa',\n",
       " 'should',\n",
       " 'media',\n",
       " 'posts',\n",
       " \"here's\",\n",
       " 'probe',\n",
       " 'time',\n",
       " 'revenue',\n",
       " 'smartphone',\n",
       " 'growth',\n",
       " 'police',\n",
       " \"twitter's\",\n",
       " 'shows',\n",
       " 'world',\n",
       " 'prime',\n",
       " 'software',\n",
       " '2',\n",
       " 'wants',\n",
       " 'home',\n",
       " 'bid',\n",
       " 'former',\n",
       " 'cook',\n",
       " 'say',\n",
       " 'record',\n",
       " 'maps',\n",
       " 'man',\n",
       " 'accounts',\n",
       " 'their',\n",
       " 'talks',\n",
       " 'ban',\n",
       " 'week',\n",
       " 'property',\n",
       " 'makes',\n",
       " '.',\n",
       " 'grapples',\n",
       " 'buys',\n",
       " 'mark',\n",
       " 'share',\n",
       " 'lawsuit',\n",
       " 'intellectual',\n",
       " 'earnings',\n",
       " 'user',\n",
       " 'iphones',\n",
       " 'office',\n",
       " 'page',\n",
       " '%',\n",
       " 'faces',\n",
       " 'battle',\n",
       " 'years',\n",
       " 'i',\n",
       " 'best',\n",
       " 'wins',\n",
       " 'show',\n",
       " 'startup',\n",
       " 'push',\n",
       " 'hit',\n",
       " 'still',\n",
       " 'own',\n",
       " 'amid',\n",
       " 'results',\n",
       " 'open',\n",
       " 'south',\n",
       " 'ios',\n",
       " 'face',\n",
       " 'car',\n",
       " 'price',\n",
       " 'judge',\n",
       " '\\x80\\x99',\n",
       " 'goes',\n",
       " 'or',\n",
       " 'fire',\n",
       " 'steve',\n",
       " 'digital',\n",
       " 'claims',\n",
       " 'group',\n",
       " 'he',\n",
       " 'rules',\n",
       " 'set',\n",
       " 'event',\n",
       " 'sell',\n",
       " 'other',\n",
       " 'seeks',\n",
       " 'two',\n",
       " 'executive',\n",
       " 'view',\n",
       " 'latest',\n",
       " '8',\n",
       " 'ahead',\n",
       " 'youtube',\n",
       " 'government',\n",
       " 'before',\n",
       " 'streaming',\n",
       " 'go',\n",
       " 'netflix',\n",
       " '4',\n",
       " 'we',\n",
       " 'work',\n",
       " 'chinese',\n",
       " 'offers',\n",
       " '7',\n",
       " 'grapple',\n",
       " 'hits',\n",
       " 'review',\n",
       " 'do',\n",
       " 'reports',\n",
       " 'using',\n",
       " 'content',\n",
       " 'site',\n",
       " 'nokia',\n",
       " 'tim',\n",
       " 'calls',\n",
       " '3',\n",
       " 'york',\n",
       " 'technology',\n",
       " 'yahoo',\n",
       " 'feature',\n",
       " 'ai',\n",
       " 'war',\n",
       " 'offer',\n",
       " 'stocks',\n",
       " 'announces',\n",
       " 'support',\n",
       " 'challenge',\n",
       " 'attack',\n",
       " 'russia',\n",
       " 'access',\n",
       " 'if',\n",
       " 'services',\n",
       " 'under',\n",
       " 'europe',\n",
       " 'photos',\n",
       " 'election',\n",
       " 'joins',\n",
       " 'change',\n",
       " 'boost',\n",
       " 'devices',\n",
       " 'project',\n",
       " 'patents',\n",
       " 'investors',\n",
       " 'update',\n",
       " 'game',\n",
       " 'brazil',\n",
       " 'german',\n",
       " 'sues',\n",
       " 'stores',\n",
       " 'russian',\n",
       " 'global',\n",
       " 'sale',\n",
       " 'glass',\n",
       " 'xbox',\n",
       " 'opens',\n",
       " '!',\n",
       " 'doodle',\n",
       " 'biggest',\n",
       " 'suit',\n",
       " 'head',\n",
       " 'french',\n",
       " 'kindle',\n",
       " 'phones',\n",
       " 'raises',\n",
       " 'high',\n",
       " 'beats',\n",
       " 'way',\n",
       " 'motorola',\n",
       " 'tweets',\n",
       " 'network',\n",
       " 'coming',\n",
       " 'stop',\n",
       " 'firm',\n",
       " 'donald',\n",
       " 'end',\n",
       " 'when',\n",
       " 'working',\n",
       " 'loses',\n",
       " 'employees',\n",
       " 'right',\n",
       " '+',\n",
       " 'look',\n",
       " 'changes',\n",
       " 'future',\n",
       " 'power',\n",
       " 'campaign',\n",
       " 'ruling',\n",
       " 'build',\n",
       " 'platform',\n",
       " 'team',\n",
       " \"won't\",\n",
       " 'reveals',\n",
       " 'sees',\n",
       " 'another',\n",
       " 'searches',\n",
       " 'being',\n",
       " 'made',\n",
       " 'again',\n",
       " 'exec',\n",
       " 'they',\n",
       " 'x',\n",
       " \"trump's\",\n",
       " 'used',\n",
       " 'reportedly',\n",
       " 'bill',\n",
       " 'making',\n",
       " 'trial',\n",
       " 'instagram',\n",
       " 'products',\n",
       " 'photo',\n",
       " 'features',\n",
       " 'health',\n",
       " 'win',\n",
       " 'surface',\n",
       " 'rival',\n",
       " 'lead',\n",
       " 'fall',\n",
       " 'board',\n",
       " 'political',\n",
       " 'her',\n",
       " 'trade',\n",
       " 'companies',\n",
       " 'so',\n",
       " 'gives',\n",
       " \"don't\",\n",
       " 'threat',\n",
       " 'reality',\n",
       " 'expands',\n",
       " 'obama',\n",
       " 'want',\n",
       " 'tool',\n",
       " 'bezos',\n",
       " 'smart',\n",
       " 'delivery',\n",
       " 'too',\n",
       " 'self-driving',\n",
       " '/',\n",
       " 'know',\n",
       " 'warns',\n",
       " 'developers',\n",
       " 'alexa',\n",
       " 'times',\n",
       " 'move',\n",
       " '--',\n",
       " 'law',\n",
       " 'product',\n",
       " 'european',\n",
       " 'advertising',\n",
       " 'echo',\n",
       " 'start',\n",
       " 'falls',\n",
       " 'briefing',\n",
       " 'friends',\n",
       " 'drop',\n",
       " '6',\n",
       " 'second',\n",
       " 'woman',\n",
       " 'house',\n",
       " 'policy',\n",
       " 'gains',\n",
       " '#',\n",
       " 'rise',\n",
       " 'white',\n",
       " 'test',\n",
       " 'going',\n",
       " 'fine',\n",
       " 'founder',\n",
       " 'cars',\n",
       " 'system',\n",
       " 'appeal',\n",
       " 'workers',\n",
       " 'cut',\n",
       " 'job',\n",
       " 'keep',\n",
       " 'better',\n",
       " 'money',\n",
       " 'must',\n",
       " 'cuts',\n",
       " 'fund',\n",
       " 'state',\n",
       " 'hires',\n",
       " 'would',\n",
       " 'customers',\n",
       " 'dispute',\n",
       " 'fix',\n",
       " 'whatsapp',\n",
       " 'city',\n",
       " 'percent',\n",
       " 'orders',\n",
       " 'lets',\n",
       " 'play',\n",
       " 'boss',\n",
       " 'looks',\n",
       " 'demand',\n",
       " 'president',\n",
       " 'target',\n",
       " 'block',\n",
       " 'whole',\n",
       " 'deals',\n",
       " 'life',\n",
       " 'estimates',\n",
       " 'brings',\n",
       " 'give',\n",
       " 'bug',\n",
       " 'nexus',\n",
       " 'asks',\n",
       " 'secret',\n",
       " 'public',\n",
       " 'need',\n",
       " 'even',\n",
       " 'behind',\n",
       " 'death',\n",
       " 'feed',\n",
       " 'order',\n",
       " 'attacks',\n",
       " 'stake',\n",
       " 'find',\n",
       " 'meet',\n",
       " 'foods',\n",
       " 'competition',\n",
       " 'updates',\n",
       " 'finds',\n",
       " 'apples',\n",
       " 'questions',\n",
       " 'row',\n",
       " 'retail',\n",
       " 'finally',\n",
       " 'through',\n",
       " 'selling',\n",
       " 'partners',\n",
       " 'fans',\n",
       " 'release',\n",
       " 'good',\n",
       " 'despite',\n",
       " 'nasdaq',\n",
       " 'drops',\n",
       " 'games',\n",
       " 'quarter',\n",
       " 'cash',\n",
       " 'only',\n",
       " 'speech',\n",
       " 'amazon.com',\n",
       " 'let',\n",
       " 'co-founder',\n",
       " 'see',\n",
       " 'virtual',\n",
       " 'cambridge',\n",
       " 'rivals',\n",
       " 'does',\n",
       " 'sells',\n",
       " 'here',\n",
       " 'major',\n",
       " 'book',\n",
       " 'turns',\n",
       " 'tells',\n",
       " 'accused',\n",
       " 'strong',\n",
       " 'legal',\n",
       " 'black',\n",
       " 'france',\n",
       " 'british',\n",
       " 'getting',\n",
       " 'scandal',\n",
       " 'today',\n",
       " 'prices',\n",
       " 'center',\n",
       " 'hate',\n",
       " 'sets',\n",
       " 'seen',\n",
       " 'releases',\n",
       " 'much',\n",
       " 'chip',\n",
       " 'chrome',\n",
       " 'study',\n",
       " 'germany',\n",
       " 'away',\n",
       " '100',\n",
       " 'them',\n",
       " 'join',\n",
       " 'design',\n",
       " 'women',\n",
       " 'supplier',\n",
       " 'assistant',\n",
       " 'things',\n",
       " 'tests',\n",
       " 'key',\n",
       " 'holiday',\n",
       " 'messenger',\n",
       " 'taking',\n",
       " 'pro',\n",
       " 'program',\n",
       " 'investment',\n",
       " 'real',\n",
       " 'soon',\n",
       " 'jeff',\n",
       " 'oracle',\n",
       " 'california',\n",
       " 'problem',\n",
       " 'debut',\n",
       " 'tools',\n",
       " 'analytica',\n",
       " 'htc',\n",
       " 'building',\n",
       " 'starts',\n",
       " \"can't\",\n",
       " 'japan',\n",
       " 'schmidt',\n",
       " 'alphabet',\n",
       " 'mac',\n",
       " 'names',\n",
       " 'got',\n",
       " \",'\",\n",
       " 'add',\n",
       " 'others',\n",
       " 'without',\n",
       " 'moves',\n",
       " '20',\n",
       " 'three',\n",
       " 'last',\n",
       " 'tops',\n",
       " 'rises',\n",
       " 'headquarters',\n",
       " 'hacked',\n",
       " 'fbi',\n",
       " 'needs',\n",
       " 'siri',\n",
       " \"what's\",\n",
       " 'voice',\n",
       " 'did',\n",
       " 'value',\n",
       " 'leave',\n",
       " 'puts',\n",
       " 'shopping',\n",
       " 'sold',\n",
       " 'during',\n",
       " 't',\n",
       " 'abuse',\n",
       " 'unit',\n",
       " 'ftc',\n",
       " 'itunes',\n",
       " 'targets',\n",
       " 'via',\n",
       " 'really',\n",
       " \"china's\",\n",
       " 'call',\n",
       " 'concerns',\n",
       " 'congress',\n",
       " 'profits',\n",
       " 'beat',\n",
       " 'buying',\n",
       " 'rolls',\n",
       " 'computer',\n",
       " 'bring',\n",
       " '500',\n",
       " \"doesn't\",\n",
       " 'issues',\n",
       " 'settlement',\n",
       " 'might',\n",
       " 'denies',\n",
       " 'close',\n",
       " 'five',\n",
       " 'early',\n",
       " 'star',\n",
       " 'local',\n",
       " 'intel',\n",
       " 'sued',\n",
       " 'huge',\n",
       " 'introduces',\n",
       " 'my',\n",
       " 'him',\n",
       " 'partner',\n",
       " '9',\n",
       " 'become',\n",
       " 'publishers',\n",
       " 'bad',\n",
       " 'acquires',\n",
       " 'daily',\n",
       " 'create',\n",
       " 'bans',\n",
       " 'expected',\n",
       " 'aims',\n",
       " 'button',\n",
       " 'rights',\n",
       " 'e-book',\n",
       " 'signs',\n",
       " 'action',\n",
       " 'vs.',\n",
       " \"isn't\",\n",
       " 'month',\n",
       " 'testing',\n",
       " 'staff',\n",
       " 'list',\n",
       " 'since',\n",
       " 'private',\n",
       " 'korea',\n",
       " 'series',\n",
       " '2016',\n",
       " 'regulators',\n",
       " 'focus',\n",
       " 'giant',\n",
       " 'following',\n",
       " 'turn',\n",
       " 'pages',\n",
       " 'north',\n",
       " 'breach',\n",
       " 'files',\n",
       " 'hackers',\n",
       " 'website',\n",
       " 'helps',\n",
       " 'unveil',\n",
       " 'put',\n",
       " 'sharing',\n",
       " 'u.k.',\n",
       " 'indian',\n",
       " 'claim',\n",
       " 'debate',\n",
       " 'messages',\n",
       " 'worth',\n",
       " 'pixel',\n",
       " \"world's\",\n",
       " 'lawmakers',\n",
       " 'been',\n",
       " 'inside',\n",
       " 'washington',\n",
       " 'name',\n",
       " 'employee',\n",
       " 'london',\n",
       " 'leads',\n",
       " 'steps',\n",
       " 'drive',\n",
       " 'where',\n",
       " 'browser',\n",
       " 'eyes',\n",
       " 'acquisition',\n",
       " 'surge',\n",
       " 'had',\n",
       " 'tablets',\n",
       " 'loss',\n",
       " 'videos',\n",
       " 'walmart',\n",
       " 'version',\n",
       " 'industry',\n",
       " 'comes',\n",
       " 'uses',\n",
       " 'san',\n",
       " 'confirms',\n",
       " 'race',\n",
       " 'trending',\n",
       " 'partnership',\n",
       " 'watchdog',\n",
       " 'maker',\n",
       " '2012',\n",
       " 'while',\n",
       " 'businesses',\n",
       " 'uber',\n",
       " 'picks',\n",
       " 'dorsey',\n",
       " 'yet',\n",
       " 'investigation',\n",
       " '50',\n",
       " 'macbook',\n",
       " 'long',\n",
       " 'requests',\n",
       " 'run',\n",
       " 'defends',\n",
       " 'ever',\n",
       " 'small',\n",
       " 'taxes',\n",
       " 'official',\n",
       " 'p',\n",
       " 'expand',\n",
       " 'analysts',\n",
       " 'removes',\n",
       " 'blocks',\n",
       " 'comments',\n",
       " 'part',\n",
       " 'regulator',\n",
       " 'role',\n",
       " 'books',\n",
       " 'days',\n",
       " 'gmail',\n",
       " 'paid',\n",
       " 'spotify',\n",
       " 'tweet',\n",
       " 'silicon',\n",
       " 'reach',\n",
       " 'jack',\n",
       " 'east',\n",
       " 'hack',\n",
       " 'sony',\n",
       " 'suppliers',\n",
       " 'sandberg',\n",
       " 'months',\n",
       " 'ready',\n",
       " 'rejects',\n",
       " 'lost',\n",
       " 'decision',\n",
       " 'complaint',\n",
       " 'dead',\n",
       " 'ibm',\n",
       " 'strategy',\n",
       " 'drone',\n",
       " 'mini',\n",
       " 'control',\n",
       " 'hq2',\n",
       " 'pressure',\n",
       " 'analyst',\n",
       " 'found',\n",
       " 'suspends',\n",
       " 'love',\n",
       " 'bigger',\n",
       " 'goldman',\n",
       " '2018',\n",
       " 'oil',\n",
       " 'energy',\n",
       " 'chat',\n",
       " 'leaves',\n",
       " 'friday',\n",
       " 'pc',\n",
       " 'consumer',\n",
       " 'minister',\n",
       " 'vote',\n",
       " 'research',\n",
       " 'developer',\n",
       " 'galaxy',\n",
       " 'conference',\n",
       " 'our',\n",
       " 'line',\n",
       " 'ireland',\n",
       " 'lower',\n",
       " 'investor',\n",
       " 'ends',\n",
       " 'seek',\n",
       " 'murder',\n",
       " 'less',\n",
       " 'advertisers',\n",
       " 'display',\n",
       " 'hardware',\n",
       " 'bank',\n",
       " 'wireless',\n",
       " 'valley',\n",
       " 'irish',\n",
       " 'device',\n",
       " 'available',\n",
       " 'settle',\n",
       " 'vr',\n",
       " 'followers',\n",
       " 'talk',\n",
       " 'markets',\n",
       " 'charges',\n",
       " 'becomes',\n",
       " 'smartphones',\n",
       " 'check',\n",
       " 'jump',\n",
       " 'firms',\n",
       " 'email',\n",
       " 'space',\n",
       " 'engine',\n",
       " 'linkedin',\n",
       " '£',\n",
       " 'meeting',\n",
       " 'challenges',\n",
       " 'fears',\n",
       " 'begins',\n",
       " 'higher',\n",
       " 'censorship',\n",
       " 'sites',\n",
       " 'ebay',\n",
       " 'wrong',\n",
       " 'cost',\n",
       " 'boosts',\n",
       " 'family',\n",
       " 'appeals',\n",
       " 'risk',\n",
       " 'details',\n",
       " 'faster',\n",
       " 'groups',\n",
       " 'banks',\n",
       " 'admits',\n",
       " 'american',\n",
       " 'personal',\n",
       " 'pays',\n",
       " '...',\n",
       " 'tell',\n",
       " 'many',\n",
       " 'australia',\n",
       " 'spending',\n",
       " 'forecast',\n",
       " 'broadcom',\n",
       " 'rally',\n",
       " 'april',\n",
       " 'popular',\n",
       " 'information',\n",
       " 'bing',\n",
       " 'mln',\n",
       " 'skype',\n",
       " 'kids',\n",
       " 'ipads',\n",
       " 'crisis',\n",
       " 'urges',\n",
       " 'screen',\n",
       " '30',\n",
       " 'campus',\n",
       " 'hold',\n",
       " 'among',\n",
       " 'images',\n",
       " '15',\n",
       " 'everything',\n",
       " 'come',\n",
       " 'intelligence',\n",
       " 'protest',\n",
       " 'every',\n",
       " 'air',\n",
       " '12',\n",
       " 'expect',\n",
       " 'arrested',\n",
       " 'hiring',\n",
       " 'costs',\n",
       " 'ask',\n",
       " 'brexit',\n",
       " 'these',\n",
       " 'debuts',\n",
       " 'threatens',\n",
       " 'brand',\n",
       " 'called',\n",
       " 'complaints',\n",
       " 'backs',\n",
       " 'around',\n",
       " 'explains',\n",
       " 'likely',\n",
       " 'me',\n",
       " 'engineer',\n",
       " 'track',\n",
       " 'teams',\n",
       " 'blocked',\n",
       " 'invest',\n",
       " 'birthday',\n",
       " 'supreme',\n",
       " 'ways',\n",
       " 'recognition',\n",
       " 'america',\n",
       " \"users'\",\n",
       " 'planning',\n",
       " 'cyber',\n",
       " 'hopes',\n",
       " 'stream',\n",
       " 'camera',\n",
       " 'party',\n",
       " 'gain',\n",
       " 'child',\n",
       " 'were',\n",
       " 'os',\n",
       " 'history',\n",
       " 'chips',\n",
       " 'tries',\n",
       " 'links',\n",
       " 'morgan',\n",
       " 'millions',\n",
       " 'jury',\n",
       " 'pulls',\n",
       " 'think',\n",
       " 'remove',\n",
       " 'because',\n",
       " 'outlook',\n",
       " 'sign',\n",
       " '2017',\n",
       " 'message',\n",
       " 'pushes',\n",
       " 'killing',\n",
       " 'between',\n",
       " '11',\n",
       " 'agency',\n",
       " 'problems',\n",
       " 'banned',\n",
       " 'messaging',\n",
       " 'little',\n",
       " 'charge',\n",
       " 'cities',\n",
       " 'save',\n",
       " 'porn',\n",
       " 'approval',\n",
       " 'christmas',\n",
       " 'ballmer',\n",
       " 'marketing',\n",
       " 'takeover',\n",
       " 'sex',\n",
       " 'battery',\n",
       " 'looking',\n",
       " 'nigeria',\n",
       " 'snapchat',\n",
       " 'fines',\n",
       " 'leader',\n",
       " 'july',\n",
       " 'which',\n",
       " 'diversity',\n",
       " 'bets',\n",
       " 'missing',\n",
       " 'payments',\n",
       " 'wal-mart',\n",
       " 'contract',\n",
       " 'backlash',\n",
       " 'u.s',\n",
       " 'great',\n",
       " 'encryption',\n",
       " 'e-books',\n",
       " 'keeps',\n",
       " 'she',\n",
       " 'posting',\n",
       " 'jumps',\n",
       " '2015',\n",
       " '25',\n",
       " 'children',\n",
       " 'quarterly',\n",
       " 'tracking',\n",
       " \"didn't\",\n",
       " 'speaker',\n",
       " 'third',\n",
       " 'school',\n",
       " 'turkey',\n",
       " 'half',\n",
       " 'growing',\n",
       " 'step',\n",
       " 'mobility',\n",
       " 'return',\n",
       " 'blackberry',\n",
       " ...]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_batch(dataset, batch_size):\n",
    "    \"\"\" Builds a batch of source and target elements from the dataset.\n",
    "    \n",
    "        Arguments:\n",
    "            dataset: List[db_element] -- A list of dataset elements\n",
    "            batch_size: int -- The size of the batch that should be created\n",
    "        Returns:\n",
    "            batch_input: List[List[int]] -- List of source sequences\n",
    "            batch_target: List[List[int]] -- List of target sequences\n",
    "            batch_target_mask: List[List[int]] -- List of target batch masks\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    # We get a list of indices we will choose from the dataset.\n",
    "    # The randint function uses a uniform distribution, giving equal probably to any entry\n",
    "    # for each batch\n",
    "    indices = list(np.random.randint(0, len(dataset), size=batch_size))\n",
    "    \n",
    "    # Recover what the entries for the batch are\n",
    "    batch = [dataset[i] for i in indices]\n",
    "    \n",
    "    # Get the raw numerized for this input, each element of the dataset has a 'numerized' key\n",
    "    batch_numerized = [dataset[i]['numerized'] for i in indices]\n",
    "\n",
    "    # Create an array of start_index that will be concatenated at position 1 for the input.\n",
    "    # Should be of shape (batch_size, 1)\n",
    "    start_tokens = np.zeros((batch_size,1))\n",
    "\n",
    "    # Concatenate the start_tokens with the rest of the input\n",
    "    # The np.concatenate function should be useful\n",
    "    # The output should now be [batch_size, sequence_length+1]\n",
    "    batch_input = np.concatenate((start_tokens,batch_numerized),axis=1)\n",
    "\n",
    "    # Remove the last word from each element in the batch\n",
    "    # To restore the [batch_size, sequence_length] size\n",
    "    batch_input = batch_input[:,:-1]\n",
    "    #print(batch_size)\n",
    "    #print(batch_input.shape)\n",
    "    \n",
    "    # The target should be the un-shifted numerized input\n",
    "    batch_target = batch_numerized\n",
    "\n",
    "    # The target-mask is a 0 or 1 filter to note which tokens are\n",
    "    # padding or not, to give the loss, so the model doesn't get rewarded for\n",
    "    # predicting PAD tokens.\n",
    "    batch_target_mask = np.array([a['mask'] for a in batch])\n",
    "    \n",
    "\n",
    "        \n",
    "    return batch_input, batch_target, batch_target_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've written the data pipelining, we are ready to write the Neural network.\n",
    "\n",
    "The steps to setting up a neural network to do Language modeling are:\n",
    "- Creating the placeholders for the model, where we can feed in our inputs and targets.\n",
    "- Creating an RNN of our choice, size, and with optional parameters\n",
    "- Using the RNN on our placeholder inputs.\n",
    "- Getting the output from the RNN, and projecting it into a vocabulary sized dimension, so that we can make word predictions.\n",
    "- Setting up the loss on the outputs so that the network learns to produce the correct words.\n",
    "- Finally, choosing an optimizer, and defining a training operation: using the optimizer to minimize the loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using a basic RNN/LSTM for Language modeling\n",
    "class LanguageModel():\n",
    "    def __init__(self, input_length, vocab_size, rnn_size, learning_rate=1e-4):\n",
    "        \n",
    "        # Create the placeholders for the inputs:\n",
    "        # All three placeholders should be of size [None, input_length]\n",
    "        # Where None represents a variable batch_size, and input_length is the\n",
    "        # maximal length of a sequence of words, after being padded.\n",
    "        self.input_num = tf.placeholder(tf.int32, shape=[None, input_length])\n",
    "        self.targets = tf.placeholder(tf.int32, shape=[None, input_length])\n",
    "        self.targets_mask = tf.placeholder(tf.int32, shape=[None, input_length])\n",
    "\n",
    "        # Create an embedding variable of shape [vocab_size, rnn_size]\n",
    "        # That will map each word in our vocab into a vector of rnn_size size.\n",
    "        embedding =tf.Variable(tf.random.normal((vocab_size,rnn_size)))\n",
    "        # Use the tensorflow embedding_lookup function\n",
    "        # To embed the input_num, using the embedding variable we've created\n",
    "        input_emb = tf.nn.embedding_lookup(embedding,self.input_num)\n",
    "\n",
    "        # Create a an RNN or LSTM cell of rnn_size size.\n",
    "        # Look into the tf.nn.rnn_cell documentation\n",
    "        # You can optionally use Tensorflow Add-ons such as the MultiRNNCell, or the DropoutWrapper\n",
    "        lm_cell = tf.nn.rnn_cell.LSTMCell(rnn_size)\n",
    "        lm_cell = tf.nn.rnn_cell.DropoutWrapper(lm_cell, output_keep_prob=0.5)\n",
    "        # Use the dynamic_rnn function of Tensorflow to run the embedded inputs\n",
    "        # using the lm_cell you've created, and obtain the outputs of the RNN cell.\n",
    "        # You have created a cell, which represents a single block (column) of the RNN.\n",
    "        # dynamic_rnn will \"copy\" the cell for each element in your sequence, runs the input you provide through the cell,\n",
    "        # and returns the outputs and the states of the cell.\n",
    "        outputs, states = tf.nn.dynamic_rnn(lm_cell, input_emb,dtype=tf.float32)\n",
    "\n",
    "        # Use a dense layer to project the outputs of the RNN cell into the size of the\n",
    "        # vocabulary (vocab_size).\n",
    "        # output_logits should be of shape [None,input_length,vocab_size]\n",
    "        self.output_logits = tf.layers.dense(outputs,vocab_size)\n",
    "\n",
    "        # Setup the loss: using the sparse_softmax_cross_entropy.\n",
    "        # The logits are the output_logits we've computed.\n",
    "        # The targets are the gold labels we are trying to match\n",
    "        # Don't forget to use the targets_mask we have, so your loss is not off,\n",
    "        # And your model doesn't get rewarded for predicting PAD tokens\n",
    "        # You might have to cast the masks into float32. Look at the tf.cast function.\n",
    "        \n",
    "        \n",
    "          \n",
    "        self.loss = tf.losses.sparse_softmax_cross_entropy(self.targets,self.output_logits,weights=self.targets_mask)\n",
    "\n",
    "        # Setup an optimizer (SGD, RMSProp, Adam)\n",
    "        \n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=5.0e-5)       \n",
    "\n",
    "        # We create a train_op that requires the optimizer we've created to minimize the\n",
    "        # loss we've defined.\n",
    "        \n",
    "        self.global_step = tf.train.get_or_create_global_step()\n",
    "        self.train_op = optimizer.minimize(self.loss,global_step=self.global_step)\n",
    "        self.saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have created the Model class, we should instantiate the model. The line tf.reset_default_graph() resets the graph for the Jupyter notebook, so multiple models aren't floating around. If you have trouble with redefinition of variables, it may be worth re-running the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can create our model,\n",
    "# with parameters of our choosing.\n",
    "\n",
    "tf.reset_default_graph() # This is so that when you debug, you reset the graph each time you run this, in essence, cleaning the board\n",
    "model = LanguageModel(input_length=input_length, vocab_size=vocab_size, rnn_size=256, learning_rate=5e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models/better_language_model\n"
     ]
    }
   ],
   "source": [
    "\n",
    "old=root_folder+\"models/better_language_model\"\n",
    "new_experiment = root_folder+\"models/final_language_model\"\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # Here is how you initialize weights of the model according to their\n",
    "    # Initialization parameters.\n",
    "    model.saver.restore(sess, old)\n",
    "    for i in range(0):\n",
    "        \n",
    "\n",
    "        # Here is how you obtain a batch:\n",
    "        batch_size = 256\n",
    "        batch_input, batch_target, batch_target_mask = build_batch(d_train, batch_size)\n",
    "        # Map the values to each tensor in a `feed_dict`\n",
    "        feed = {model.input_num: batch_input, model.targets: batch_target, model.targets_mask: batch_target_mask}\n",
    "\n",
    "        # Obtain a single value of the loss for that batch.\n",
    "        step, train_loss, _ = sess.run([model.global_step, model.loss, model.train_op], feed_dict=feed)\n",
    "        if i%10==0:\n",
    "            print(step)\n",
    "            print(train_loss)\n",
    "    # Here is how you save the model weights\n",
    "    model.saver.save(sess, new_experiment)\n",
    "    \n",
    "#     # Here is how you restore the weights previously saved\n",
    "#     model.saver.restore(sess, experiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations, you have now trained a language model! We can now use it to evaluate likely news headlines, as well as generate our very own headlines.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) Evaluation loss\n",
    "\n",
    "To evaluate the language model, we evaluate its loss (ability to predict) on unseen data that is reserved for evaluation.\n",
    "Your first evaluation is to load the model you trained, and obtain a test loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = root_folder+\"models/final_language_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models/final_language_model\n",
      "Evaluation set loss: [5.5446362]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with tf.Session() as sess:\n",
    "    model.saver.restore(sess, model_file)\n",
    "    eval_input, eval_target, eval_target_mask = build_batch(d_valid, 500)\n",
    "    #print(eval_input,eval_target)\n",
    "    feed = {model.input_num: eval_input, model.targets: eval_target, model.targets_mask: eval_target_mask}\n",
    "    eval_loss = sess.run([model.loss], feed_dict=feed)\n",
    "    print(\"Evaluation set loss:\", eval_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Evaluation of likelihood of data\n",
    "\n",
    "One use of a language model is to see what data is more likely to have originated from the training data. Because we have trained our model on news headlines, we can see which of these headlines is more likely:\n",
    "\n",
    "``Apple to release another iPhone in September``\n",
    "\n",
    "\n",
    " ``Apple and Samsung resolve all lawsuits amicably``\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first one is obviously more likely since iphones are released yearly but apple and samsung will never amicably resolve lawsuits due to money and competition struggles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models/final_language_model\n",
      "----------------------------------------\n",
      "Headline: apple to release new iphone in july\n",
      "Loss of the headline: 3.5208185\n",
      "----------------------------------------\n",
      "Headline: apple and samsung resolve all lawsuits\n",
      "Loss of the headline: 5.736441\n"
     ]
    }
   ],
   "source": [
    "headline1 = \"Apple to release new iPhone in July\"\n",
    "headline2 = \"Apple and Samsung resolve all lawsuits\"\n",
    "import numpy as np\n",
    "headlines = [headline1, headline2]\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    model.saver.restore(sess, model_file)\n",
    "\n",
    "    for headline in headlines:\n",
    "        headline = headline.lower() # Our LSTM is trained on lower-cased headlines\n",
    "    \n",
    "        # From the code in the Preprocessing section at the end of the notebook\n",
    "        # Find out how to tokenize the headline\n",
    "        tokenized =tknzr.tokenize(headline)\n",
    "        \n",
    "        # Find out how to numerize the tokenized headline\n",
    "        numerized = numerize_sequence(tokenized)\n",
    "\n",
    "        # Learn how to pad and obtain the mask of the sequence.\n",
    "        padded, mask = pad_sequence(numerized, len(numerized), 20)\n",
    "        #print(tokenized,numerized,padded,mask)\n",
    "        \n",
    "        # Obtain the loss of the sequence, and pring it\n",
    "        feed = {model.input_num: np.array([0]+padded[:-1]).reshape(1,20), model.targets: np.array(padded).reshape(1,20), model.targets_mask: np.array(mask).reshape(1,20)}\n",
    "        loss,outputlogits = sess.run([model.loss,model.output_logits], feed_dict=feed)\n",
    "#         print(outputlogits)\n",
    "#         for i in outputlogits[0]:\n",
    "#             print(np.argmax(i))\n",
    "        print(\"----------------------------------------\")\n",
    "        print(\"Headline:\",headline)\n",
    "        print(\"Loss of the headline:\", loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) Generation of headlines\n",
    "\n",
    "We can use our language model to generate text according to the distribution of our training data.\n",
    "The way generation works is the following:\n",
    "\n",
    "We seed the model with a beginning of sequence, and obtain the distribution for the next word.\n",
    "We select the most likely word (argmax) and add it to our sequence of words.\n",
    "Now our sequence is one word longer, and we can feed it in again as an input, for the network to produce the next sentence.\n",
    "We do this a fixed number of times (up to 20 words), and obtain automatically generated headlines!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models/final_language_model\n",
      "===================\n",
      "Generating headline starting with: apple has released\n",
      "<START> apple has released a UNK to the iphone 8 , but it will be a UNK UNK for the\n",
      "===================\n",
      "Generating headline starting with: google has released\n",
      "<START> google has released a new version of the UNK of the new york city , UNK and the cloud\n",
      "===================\n",
      "Generating headline starting with: amazon\n",
      "<START> amazon is launching a new version of the cloud computing ? here's what it means to the UNK of\n",
      "===================\n",
      "Generating headline starting with: tesla to\n",
      "<START> tesla to buy apple , UNK UNK , UNK UNK , UNK , UNK , UNK , est. UNK\n",
      "===================\n",
      "Generating headline starting with: google and apple\n",
      "<START> google and apple are UNK up to UNK the UNK UNK . UNK UNK UNK UNK UNK UNK UNK\n",
      "===================\n",
      "Generating headline starting with: tesla sues amazon\n",
      "<START> tesla sues amazon to block UNK of UNK iphone sales ban on iphone sales , UNK says apple is\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    model.saver.restore(sess, model_file)\n",
    "\n",
    "    # Here are some headline starters.\n",
    "    # They're all about tech companies, because\n",
    "    # That is what is in our dataset\n",
    "    headline_starters = [\"apple has released\", \"google has released\", \"amazon\", \"tesla to\", \"google and apple\", \"tesla sues amazon\"]\n",
    "    \n",
    "    for headline_starter in headline_starters:\n",
    "        print(\"===================\")\n",
    "        print(\"Generating headline starting with: \"+headline_starter)\n",
    "\n",
    "        # Tokenize and numerize the headline. Put the numerized headline\n",
    "        # beginning in `current_build`\n",
    "        tokenized = tokenized =tknzr.tokenize(headline_starter)\n",
    "        current_build = [startI] + numerize_sequence(tokenized)\n",
    "    \n",
    "        while len(current_build) < input_length:\n",
    "            # Pad the current_build into a input_length vector.\n",
    "            # We do this so that it can be processed by our LanguageModel class\n",
    "            current_padded = current_build[:input_length] + [padI] * (input_length - len(current_build))\n",
    "            padded_out=current_padded[1:]+[2]\n",
    "            padded_out=np.array([padded_out])\n",
    "            current_padded = np.array([current_padded])\n",
    "            #print(current_padded)\n",
    "            # Obtain the logits for the current padded sequence\n",
    "            # This involves obtaining the output_logits from our model,\n",
    "            # and not the loss like we have done so far\n",
    "            \n",
    "            feed = {model.input_num: current_padded.reshape(1,input_length), model.targets: padded_out.reshape(1,input_length), model.targets_mask: np.array(mask).reshape(1,input_length)}\n",
    "            logits = sess.run([model.output_logits], feed_dict=feed)\n",
    "            #print(logits)\n",
    "            # Obtain the row of logits that interest us, the logits for the last non-pad\n",
    "            # inputs\n",
    "            #print(logits)\n",
    "            logits = logits[-1][0]\n",
    "            #print(logits)\n",
    "            #print(logits)\n",
    "            \n",
    "            last_logits=logits[len(current_build)-1]\n",
    "            #print(last_logits)\n",
    "            \n",
    "            #print(np.argmax(last_logits))\n",
    "            # Find the highest scoring word in the last_logits\n",
    "            # array. The np.argmax function should be useful.\n",
    "            # Append this word to our current build\n",
    "            \n",
    "            current_build.append(np.argmax(last_logits))\n",
    "        \n",
    "        # Go from the current_build of word_indices\n",
    "        # To the headline (string) produced. This should involve\n",
    "        # the vocabulary, and a string merger.\n",
    "        produced_sentence = numerized2text(current_build)\n",
    "        print(produced_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All done\n",
    "\n",
    "You are done with the first part of the repo\n",
    "\n",
    "\n",
    "Next notebook deals with Summarization of text!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
